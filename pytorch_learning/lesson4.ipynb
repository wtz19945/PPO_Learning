{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0680, -0.0892,  0.0117,  ...,  0.0187, -0.0285, -0.0270],\n",
      "        [-0.0661, -0.0491,  0.0674,  ...,  0.0084,  0.0950,  0.0275],\n",
      "        [ 0.0768,  0.0416,  0.0238,  ..., -0.0043,  0.0086,  0.0136],\n",
      "        ...,\n",
      "        [ 0.0591, -0.0660,  0.0782,  ..., -0.0105,  0.0391, -0.0868],\n",
      "        [ 0.0016, -0.0743, -0.0798,  ...,  0.0945,  0.0648, -0.0100],\n",
      "        [-0.0920, -0.0801, -0.0540,  ..., -0.0291,  0.0830, -0.0731]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0417, -0.0997, -0.0248,  0.0412, -0.0469,  0.0960, -0.0787,  0.0725,\n",
      "         0.0145,  0.0833, -0.0734, -0.0133,  0.0605, -0.0339,  0.0222,  0.0737,\n",
      "         0.0654,  0.0881,  0.0470,  0.0427, -0.0591,  0.0623, -0.0440, -0.0232,\n",
      "         0.0286,  0.0593, -0.0883,  0.0084,  0.0425,  0.0210, -0.0562, -0.0664,\n",
      "        -0.0588, -0.0493,  0.0274,  0.0528,  0.0176,  0.0870,  0.0411,  0.0471,\n",
      "         0.0264, -0.0424,  0.0208, -0.0332, -0.0249, -0.0825, -0.0490, -0.0729,\n",
      "         0.0153, -0.0594, -0.0287, -0.0460, -0.0478,  0.0280, -0.0472,  0.0033,\n",
      "         0.0417, -0.0985,  0.0541,  0.0073, -0.0474,  0.0461,  0.0272, -0.0730,\n",
      "        -0.0382,  0.0286,  0.0325,  0.0976, -0.0574,  0.0830, -0.0176, -0.0807,\n",
      "         0.0089, -0.0182, -0.0011,  0.0446, -0.0100, -0.0914, -0.0141,  0.0505,\n",
      "         0.0606, -0.0990, -0.0012, -0.0498, -0.0461, -0.0747, -0.0372,  0.0165,\n",
      "        -0.0515, -0.0516,  0.0385,  0.0555,  0.0880,  0.0564,  0.0143,  0.0237,\n",
      "         0.0770,  0.0093, -0.0629, -0.0336, -0.0411, -0.0506,  0.0491, -0.0584,\n",
      "        -0.0385, -0.0946,  0.0295,  0.0520,  0.0313,  0.0026, -0.0163, -0.0925,\n",
      "         0.0130,  0.0240, -0.0058, -0.0954, -0.0270,  0.0435, -0.0523, -0.0277,\n",
      "        -0.0347, -0.0235,  0.0223,  0.0153, -0.0987, -0.0355, -0.0747,  0.0075,\n",
      "         0.0536,  0.0645,  0.0215, -0.0332, -0.0142,  0.0202,  0.0358,  0.0279,\n",
      "         0.0764,  0.0574, -0.0655, -0.0233,  0.0011, -0.0980, -0.0507,  0.0499,\n",
      "         0.0487,  0.0917,  0.0548, -0.0887, -0.0338, -0.0734, -0.0472, -0.0843,\n",
      "        -0.0515, -0.0436, -0.0319,  0.0534,  0.0205, -0.0156, -0.0785, -0.0612,\n",
      "        -0.0568,  0.0594, -0.0531,  0.0406,  0.0035,  0.0894,  0.0047, -0.0643,\n",
      "        -0.0827,  0.0577,  0.0205, -0.0625,  0.0242, -0.0449,  0.0326, -0.0694,\n",
      "        -0.0326, -0.0308, -0.0154, -0.0872, -0.0009,  0.0608, -0.0465,  0.0799,\n",
      "        -0.0561, -0.0393,  0.0378, -0.0648, -0.0743, -0.0584,  0.0851,  0.0129,\n",
      "        -0.0005,  0.0125,  0.0477, -0.0335,  0.0096,  0.0034, -0.0333, -0.0753],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0342,  0.0347,  0.0295,  ...,  0.0220,  0.0082, -0.0129],\n",
      "        [-0.0091, -0.0407, -0.0023,  ...,  0.0655,  0.0190,  0.0430],\n",
      "        [ 0.0085,  0.0167,  0.0682,  ..., -0.0460, -0.0393, -0.0466],\n",
      "        ...,\n",
      "        [ 0.0432, -0.0288, -0.0036,  ...,  0.0436,  0.0596, -0.0043],\n",
      "        [ 0.0345,  0.0651,  0.0515,  ...,  0.0365,  0.0174, -0.0311],\n",
      "        [ 0.0396,  0.0685,  0.0610,  ..., -0.0187, -0.0100, -0.0021]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0650,  0.0223,  0.0591, -0.0623, -0.0625, -0.0083,  0.0032,  0.0330,\n",
      "        -0.0189,  0.0616], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0342,  0.0347,  0.0295,  ...,  0.0220,  0.0082, -0.0129],\n",
      "        [-0.0091, -0.0407, -0.0023,  ...,  0.0655,  0.0190,  0.0430],\n",
      "        [ 0.0085,  0.0167,  0.0682,  ..., -0.0460, -0.0393, -0.0466],\n",
      "        ...,\n",
      "        [ 0.0432, -0.0288, -0.0036,  ...,  0.0436,  0.0596, -0.0043],\n",
      "        [ 0.0345,  0.0651,  0.0515,  ...,  0.0365,  0.0174, -0.0311],\n",
      "        [ 0.0396,  0.0685,  0.0610,  ..., -0.0187, -0.0100, -0.0021]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0650,  0.0223,  0.0591, -0.0623, -0.0625, -0.0083,  0.0032,  0.0330,\n",
      "        -0.0189,  0.0616], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class TinyModel(torch.nn.Module):\n",
    "# Need to define an init function and a forward function\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Linear Layers\n",
      "Input:\n",
      "tensor([[0.1943, 0.8943, 0.1394]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.1199,  0.4767,  0.0901],\n",
      "        [-0.4586,  0.4353, -0.1120]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1411, 0.4884], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[0.5566, 0.7729]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Common used layers\n",
    "\n",
    "print('\\n\\n Linear Layers')\n",
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Conv Layers\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n Conv Layers')\n",
    "\n",
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " RNN Layers\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n RNN Layers')\n",
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3398, 0.6996, 0.6616, 0.1358, 0.6247, 0.9228],\n",
      "         [0.6228, 0.2028, 0.5540, 0.3011, 0.5537, 0.7178],\n",
      "         [0.1623, 0.4285, 0.6397, 0.3661, 0.3067, 0.8791],\n",
      "         [0.0626, 0.5759, 0.4374, 0.7948, 0.7028, 0.6805],\n",
      "         [0.1293, 0.8839, 0.0658, 0.9522, 0.6942, 0.1574],\n",
      "         [0.6561, 0.3933, 0.2394, 0.1451, 0.5156, 0.4543]]])\n",
      "tensor([[[0.6996, 0.9228],\n",
      "         [0.8839, 0.9522]]])\n",
      "tensor([[[18.3486, 19.5615, 15.3849,  5.3391],\n",
      "         [ 8.1349, 20.1791, 22.7153,  9.0583],\n",
      "         [21.7231, 10.9834, 14.4928, 22.9845],\n",
      "         [12.1806, 21.1385, 20.6928, 19.6607]]])\n",
      "tensor(16.4111)\n",
      "tensor(69.2585)\n",
      "tensor([[[ 0.6600,  0.8770,  0.1299, -1.6669],\n",
      "         [-1.0602,  0.7939,  1.1844, -0.9181],\n",
      "         [ 0.8379, -1.3164, -0.6124,  1.0909],\n",
      "         [-1.7132,  0.7472,  0.6247,  0.3413]]],\n",
      "       grad_fn=<NativeBatchNormBackward>)\n",
      "tensor(-4.0978e-08, grad_fn=<MeanBackward0>)\n",
      "tensor(4.0000, grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Other layers\n",
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))\n",
    "\n",
    "# Normalize layer: \n",
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "print(my_tensor.norm())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())\n",
    "print(normed_tensor.norm())\n",
    "\n",
    "# Dropout Layer\n",
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))\n",
    "\n",
    "# Activation Functions: ReLU and its many variants, Tanh, Hardtanh, sigmoid, and more.\n",
    "\n",
    "# Loss Functions: Loss functions tell us how far a modelâ€™s prediction is from the correct answer. \n",
    "# PyTorch contains a variety of loss functions, including common MSE (mean squared error = L2 norm)\n",
    "# Cross Entropy Loss and Negative Likelihood Loss (useful for classifiers), and others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppoexample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
